{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>datetime_of_event</th>\n",
       "      <th>date_submitted</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>state2</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>long_description</th>\n",
       "      <th>shape</th>\n",
       "      <th>short_description</th>\n",
       "      <th>topic</th>\n",
       "      <th>attachments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MUFON</td>\n",
       "      <td>2010-08-14 9:15PM</td>\n",
       "      <td>3/13/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bogalusa</td>\n",
       "      <td>LA</td>\n",
       "      <td>LOUISIANA</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Driving home at dark we’re on a new road that...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it was a bright blue light flying over head</td>\n",
       "      <td>blinding</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MUFON</td>\n",
       "      <td>2021-02-22 10:22PM</td>\n",
       "      <td>3/12/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sarasota</td>\n",
       "      <td>FL</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I was taking pictures of the beautiful sunris...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crystal Clear night. Looked up at Orion. Watch...</td>\n",
       "      <td>blinding</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUFON</td>\n",
       "      <td>2020-07-26 7:25PM</td>\n",
       "      <td>3/6/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hillsboro</td>\n",
       "      <td>TN</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I was driving on 285 South at 845 PM on my wa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Looked like Venus does in the evening but was ...</td>\n",
       "      <td>blinding</td>\n",
       "      <td>DE50931F02C441D3BF88C950D96C6149.jpeg trim.74A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MUFON</td>\n",
       "      <td>1996-02-20 6:00PM</td>\n",
       "      <td>2/27/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elizabethtown</td>\n",
       "      <td>KY</td>\n",
       "      <td>KENTUCKY</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My husband and I saw a red orb in the sky. It...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Huge brilliant lights</td>\n",
       "      <td>blinding</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MUFON</td>\n",
       "      <td>2021-02-25 6:42AM</td>\n",
       "      <td>2/25/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Torrington</td>\n",
       "      <td>CT</td>\n",
       "      <td>CONNECTICUT</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Was driving home from doctor appt. on busy in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Done like with apparent windows.</td>\n",
       "      <td>blinding</td>\n",
       "      <td>7CD98D9DF4464E5EB8D138AC87703BCF.jpeg 4FD788F9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source   datetime_of_event date_submitted duration_(seconds)           city  \\\n",
       "1  MUFON   2010-08-14 9:15PM        3/13/21                NaN       Bogalusa   \n",
       "2  MUFON  2021-02-22 10:22PM        3/12/21                NaN       Sarasota   \n",
       "3  MUFON   2020-07-26 7:25PM         3/6/21                NaN      Hillsboro   \n",
       "4  MUFON   1996-02-20 6:00PM        2/27/21                NaN  Elizabethtown   \n",
       "5  MUFON   2021-02-25 6:42AM        2/25/21                NaN     Torrington   \n",
       "\n",
       "  state       state2 country latitude longitude  \\\n",
       "1   LA     LOUISIANA     USA      NaN       NaN   \n",
       "2   FL       FLORIDA     USA      NaN       NaN   \n",
       "3   TN     TENNESSEE     USA      NaN       NaN   \n",
       "4   KY      KENTUCKY     USA      NaN       NaN   \n",
       "5   CT   CONNECTICUT     USA      NaN       NaN   \n",
       "\n",
       "                                    long_description shape  \\\n",
       "1   Driving home at dark we’re on a new road that...   NaN   \n",
       "2   I was taking pictures of the beautiful sunris...   NaN   \n",
       "3   I was driving on 285 South at 845 PM on my wa...   NaN   \n",
       "4   My husband and I saw a red orb in the sky. It...   NaN   \n",
       "5   Was driving home from doctor appt. on busy in...   NaN   \n",
       "\n",
       "                                   short_description     topic  \\\n",
       "1        it was a bright blue light flying over head  blinding   \n",
       "2  crystal Clear night. Looked up at Orion. Watch...  blinding   \n",
       "3  Looked like Venus does in the evening but was ...  blinding   \n",
       "4                              Huge brilliant lights  blinding   \n",
       "5                   Done like with apparent windows.  blinding   \n",
       "\n",
       "                                         attachments  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  DE50931F02C441D3BF88C950D96C6149.jpeg trim.74A...  \n",
       "4                                                NaN  \n",
       "5  7CD98D9DF4464E5EB8D138AC87703BCF.jpeg 4FD788F9...  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "# read the data in a dataframe using Pandas\n",
    "df = pd.read_csv(\"data/mufon_data.csv\")\n",
    "# the first row is the header row, use it to name the columns of the dataframe\n",
    "df.columns = df.iloc[0]\n",
    "# lowercase the column names and replace spaces with underscores, also remove non-alphanumeric characters from the column names\n",
    "df.columns = [x.lower().replace(\" \", \"_\").replace(\"/\", \"\").replace(r\"()\",\"\") for x in df.columns]\n",
    "# drop the first row, which is now a duplicate of the header row\n",
    "df = df.drop(df.index[0])\n",
    "# separate city,state into two columns\n",
    "df[[\"city\", \"state\"]] = df[\"city,state\"].str.split(\",\", expand=True)\n",
    "# drop the city,state column\n",
    "df = df.drop(\"city,state\", axis=1)\n",
    "# remove \"Long Description of Sighting Report\" from all text in the rows of the \"description\" column\n",
    "df[\"long_description\"] = df[\"long_description\"].str.replace(\"Long Description of Sighting Report\", \"\")\n",
    "# show the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv(\"data/mufon_data.csv\")\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \n",
    "    # the first row is the header row, use it to name the columns of the dataframe\n",
    "    df.columns = df.iloc[0]\n",
    "    \n",
    "    # lowercase the column names and replace spaces with underscores, also remove non-alphanumeric characters from the column names\n",
    "    df.columns = [x.lower().replace(\" \", \"_\").replace(\"/\", \"\").replace(r\"()\",\"\") for x in df.columns]\n",
    "\n",
    "    # convert datetime_of_event column to datetime object\n",
    "    df['datetime_of_event'] = pd.to_datetime(df['datetime_of_event'], errors='coerce')\n",
    "    \n",
    "    # remove 'long description..'\n",
    "    df[\"long_description\"] = df[\"long_description\"].str.replace(\"Long Description of Sighting Report\", \"\")\n",
    "\n",
    "    # drop rows with missing datetime_of_event values\n",
    "    df.dropna(subset=['datetime_of_event'], inplace=True)\n",
    "    \n",
    "    # fill missing values in duration_(seconds) column with 0\n",
    "    df['duration_(seconds)'].fillna(0, inplace=True)\n",
    "    \n",
    "    # fill missing values in latitude and longitude columns with median values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    try:\n",
    "        df[['latitude', 'longitude']] = imputer.fit_transform(df[['latitude', 'longitude']])\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # fill missing values in state column with 'unknown'\n",
    "    df['state'].fillna('unknown', inplace=True)\n",
    "    \n",
    "    # fill missing values in shape column with 'unknown'\n",
    "    df['shape'].fillna('unknown', inplace=True)\n",
    "    \n",
    "    # fill missing values in attachments column with 'unknown'\n",
    "    df['attachments'].fillna('none', inplace=True)\n",
    "    \n",
    "    # if the value in latitude, longitude is np.NaN, or duration_(seconds) is 0, replace it with the string \"pending_nuforc\"\n",
    "    df['latitude'] = np.where(df['latitude'].isnull(), np.NaN, df['latitude'])\n",
    "    df['longitude'] = np.where(df['longitude'].isnull(), np.NaN, df['longitude'])\n",
    "    df['duration_(seconds)'] = np.where(df['duration_(seconds)'] == 0, np.NaN, df['duration_(seconds)'])\n",
    "    # separate city,state into two columns\n",
    "    df[[\"city\", \"state\"]] = df[\"city,state\"].str.split(\",\", expand=True)\n",
    "    # drop the city,state column\n",
    "    df = df.drop(\"city,state\", axis=1)\n",
    "    # encode categorical columns\n",
    "    categorical_cols = ['source', 'city', 'state', 'state2', 'country', 'shape', 'topic', 'attachments']\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    # drop duplicated columns\n",
    "    df = df.drop(['state2'], axis=1)\n",
    "    # save a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "    # save as `data/mufon_data_preprocessed.csv`\n",
    "    df_copy.to_csv('data/mufon_data_preprocessed.csv', index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>shape</th>\n",
       "      <th>topic</th>\n",
       "      <th>attachments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>413.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>413.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>413.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178.622276</td>\n",
       "      <td>22.629540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.002421</td>\n",
       "      <td>90.472155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.517536</td>\n",
       "      <td>15.032192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.590591</td>\n",
       "      <td>28.793664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>134.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       source  duration_(seconds)        city       state  country  shape  \\\n",
       "count   413.0                 0.0  413.000000  413.000000    413.0  413.0   \n",
       "mean      0.0                 NaN  178.622276   22.629540      0.0    0.0   \n",
       "std       0.0                 NaN  104.517536   15.032192      0.0    0.0   \n",
       "min       0.0                 NaN    0.000000    0.000000      0.0    0.0   \n",
       "25%       0.0                 NaN   90.000000    8.000000      0.0    0.0   \n",
       "50%       0.0                 NaN  181.000000   22.000000      0.0    0.0   \n",
       "75%       0.0                 NaN  270.000000   36.000000      0.0    0.0   \n",
       "max       0.0                 NaN  360.000000   48.000000      0.0    0.0   \n",
       "\n",
       "            topic  attachments  \n",
       "count  413.000000   413.000000  \n",
       "mean     4.002421    90.472155  \n",
       "std      2.590591    28.793664  \n",
       "min      0.000000     0.000000  \n",
       "25%      2.000000    97.000000  \n",
       "50%      4.000000   103.000000  \n",
       "75%      6.000000   103.000000  \n",
       "max      8.000000   134.000000  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess the data\n",
    "df = preprocess_data(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 413 entries, 1 to 416\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   source              413 non-null    int64         \n",
      " 1   datetime_of_event   413 non-null    datetime64[ns]\n",
      " 2   date_submitted      413 non-null    object        \n",
      " 3   duration_(seconds)  0 non-null      float64       \n",
      " 4   city                413 non-null    int64         \n",
      " 5   state               413 non-null    int64         \n",
      " 6   country             413 non-null    int64         \n",
      " 7   latitude            0 non-null      object        \n",
      " 8   longitude           0 non-null      object        \n",
      " 9   long_description    413 non-null    object        \n",
      " 10  shape               413 non-null    int64         \n",
      " 11  short_description   413 non-null    object        \n",
      " 12  topic               413 non-null    int64         \n",
      " 13  attachments         413 non-null    int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(7), object(5)\n",
      "memory usage: 48.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use np.where to replace all \"pending_nuforc\" values with np.NaN in the latitude, longitude, and duration_(seconds) columns\n",
    "df['latitude'] = np.where(df['latitude'] == 'pending_nuforc', np.NaN, df['latitude'])\n",
    "df['longitude'] = np.where(df['longitude'] == 'pending_nuforc', np.NaN, df['longitude'])\n",
    "df['duration_(seconds)'] = np.where(df['duration_(seconds)'] == 'pending_nuforc', np.NaN, df['duration_(seconds)'])\n",
    "\n",
    "# finally, for every column if there are any rows with \"pending_nuforc\" values, replace those values with np.NaN\n",
    "df = df.replace('pending_nuforc', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>datetime_of_event</th>\n",
       "      <th>date_submitted</th>\n",
       "      <th>duration_(seconds)</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>long_description</th>\n",
       "      <th>shape</th>\n",
       "      <th>short_description</th>\n",
       "      <th>topic</th>\n",
       "      <th>attachments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-08-14 21:15:00</td>\n",
       "      <td>3/13/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Driving home at dark we’re on a new road that...</td>\n",
       "      <td>0</td>\n",
       "      <td>it was a bright blue light flying over head</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-22 22:22:00</td>\n",
       "      <td>3/12/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I was taking pictures of the beautiful sunris...</td>\n",
       "      <td>0</td>\n",
       "      <td>crystal Clear night. Looked up at Orion. Watch...</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-07-26 19:25:00</td>\n",
       "      <td>3/6/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I was driving on 285 South at 845 PM on my wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>Looked like Venus does in the evening but was ...</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1996-02-20 18:00:00</td>\n",
       "      <td>2/27/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My husband and I saw a red orb in the sky. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Huge brilliant lights</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-25 06:42:00</td>\n",
       "      <td>2/25/21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Was driving home from doctor appt. on busy in...</td>\n",
       "      <td>0</td>\n",
       "      <td>Done like with apparent windows.</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source   datetime_of_event date_submitted  duration_(seconds)  city  state  \\\n",
       "1       0 2010-08-14 21:15:00        3/13/21                 NaN    31     17   \n",
       "2       0 2021-02-22 22:22:00        3/12/21                 NaN   271      8   \n",
       "3       0 2020-07-26 19:25:00         3/6/21                 NaN   121     39   \n",
       "4       0 1996-02-20 18:00:00        2/27/21                 NaN    90     16   \n",
       "5       0 2021-02-25 06:42:00        2/25/21                 NaN   300      6   \n",
       "\n",
       "   country latitude longitude  \\\n",
       "1        0      NaN       NaN   \n",
       "2        0      NaN       NaN   \n",
       "3        0      NaN       NaN   \n",
       "4        0      NaN       NaN   \n",
       "5        0      NaN       NaN   \n",
       "\n",
       "                                    long_description  shape  \\\n",
       "1   Driving home at dark we’re on a new road that...      0   \n",
       "2   I was taking pictures of the beautiful sunris...      0   \n",
       "3   I was driving on 285 South at 845 PM on my wa...      0   \n",
       "4   My husband and I saw a red orb in the sky. It...      0   \n",
       "5   Was driving home from doctor appt. on busy in...      0   \n",
       "\n",
       "                                   short_description  topic  attachments  \n",
       "1        it was a bright blue light flying over head      1          103  \n",
       "2  crystal Clear night. Looked up at Orion. Watch...      1          103  \n",
       "3  Looked like Venus does in the evening but was ...      1           49  \n",
       "4                              Huge brilliant lights      1          103  \n",
       "5                   Done like with apparent windows.      1           35  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove \"Long Description of Sighting Report\" from all text in the rows of the \"description\" column\n",
    "df[\"long_description\"] = df[\"long_description\"].str.replace(\"Long Description of Sighting Report\", \"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the \"date_submitted\", and \"datetime_of_event\" columns to datetime objects\n",
    "df[\"date_submitted\"] = pd.to_datetime(df[\"date_submitted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline - in Development\n",
      "------------------------\n",
      "Current df has columns: Index(['source', 'datetime_of_event', 'date_submitted', 'duration_(seconds)',\n",
      "       'city', 'state', 'country', 'latitude', 'longitude', 'long_description',\n",
      "       'shape', 'short_description', 'topic', 'attachments'],\n",
      "      dtype='object')\n",
      "Created text_pipeline\n",
      "------------------------\n",
      "Defining the transformers\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/groupme/lib/python3.9/site-packages/sklearn/utils/extmath.py:981: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/opt/anaconda3/envs/groupme/lib/python3.9/site-packages/sklearn/utils/extmath.py:986: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/opt/anaconda3/envs/groupme/lib/python3.9/site-packages/sklearn/utils/extmath.py:1006: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "# You can now proceed to the next step of your project which is to build a model to cluster the reports, considering them like points on a plane. The plane will have 3 dimensions: time, location (lat,long). The clusters will be the groups of reports that are similar to each other. This introduces another dimension to the plane: the shape of the UAPs. The clusters will be the groups of reports that are similar to each other after the text has been vectorized with TFIDF and the shape of the UAPs has been one-hot encoded.\n",
    "# I want to build a preprocessing pipeline for the text in the reports using `pipeline` in `sklearn`\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"data/mufon_data_preprocessed.csv\")\n",
    "\n",
    "def extract_time_features(x):\n",
    "    try:\n",
    "        return x['datetime_of_event'].dt.year.astype(float) + x['datetime_of_event'].dt.month.astype(float)/12 + x['datetime_of_event'].dt.day.astype(float)/365\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "print(f'Pipeline - in Development')\n",
    "print(f'------------------------')\n",
    "print(f'Current df has columns: {df.columns}')\n",
    "\n",
    "#create the text preprocessing pipeline\n",
    "text_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()), # vectorize the text using Tf-idf\n",
    "    ])\n",
    "print(f'Created text_pipeline')\n",
    "print(f'------------------------')\n",
    "print(f'Defining the transformers')\n",
    "print(f'------------------------')\n",
    "transformers=[\n",
    "    ('text', text_pipeline, \"long_description\"), # apply the text pipeline to the long_description column\n",
    "    ('encoder', OneHotEncoder(), ['shape']), # one-hot encode the shape column\n",
    "    ('num', StandardScaler(), ['latitude', 'longitude', 'duration_(seconds)']), # scale the latitude, longitude, and duration_(seconds) columns\n",
    "]\n",
    "# create the preprocessor\n",
    "preprocessor = ColumnTransformer(transformers=transformers)\n",
    "# fit and transform the data using the preprocessor\n",
    "X_transformed = preprocessor.fit_transform(df)\n",
    "# Next step - build a model to cluster the reports, considering them like points on a plane. The plane will have 3 dimensions: time, location (lat,long). The clusters will be the groups of reports that are similar to each other. This introduces another dimension to the plane: the shape of the UAPs. The clusters will be the groups of reports that are similar to each other after the text has been vectorized with TFIDF and the shape of the UAPs has been one-hot encoded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_transformed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# get the labels\u001b[39;00m\n\u001b[1;32m     66\u001b[0m labels \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m/opt/anaconda3/envs/groupme/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1367\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1342\u001b[0m     \u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \n\u001b[1;32m   1344\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1367\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1368\u001b[0m         X,\n\u001b[1;32m   1369\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1370\u001b[0m         dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[1;32m   1371\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1372\u001b[0m         copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[1;32m   1373\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1374\u001b[0m     )\n\u001b[1;32m   1376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params(X)\n\u001b[1;32m   1377\u001b[0m     random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/groupme/lib/python3.9/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/groupme/lib/python3.9/site-packages/sklearn/utils/validation.py:822\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39mif\u001b[39;00m sp\u001b[39m.\u001b[39missparse(array):\n\u001b[1;32m    821\u001b[0m     _ensure_no_complex_data(array)\n\u001b[0;32m--> 822\u001b[0m     array \u001b[39m=\u001b[39m _ensure_sparse_format(\n\u001b[1;32m    823\u001b[0m         array,\n\u001b[1;32m    824\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m    825\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    826\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    827\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m    828\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m    829\u001b[0m         estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    830\u001b[0m         input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    831\u001b[0m     )\n\u001b[1;32m    832\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[39m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[1;32m    834\u001b[0m     \u001b[39m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[1;32m    835\u001b[0m     \u001b[39m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[1;32m    836\u001b[0m     \u001b[39m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[1;32m    837\u001b[0m     \u001b[39m# of warnings context manager.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m     \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/groupme/lib/python3.9/site-packages/sklearn/utils/validation.py:551\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    546\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    547\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt check \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m sparse matrix for nan or inf.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m spmatrix\u001b[39m.\u001b[39mformat,\n\u001b[1;32m    548\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m    549\u001b[0m         )\n\u001b[1;32m    550\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m         _assert_all_finite(\n\u001b[1;32m    552\u001b[0m             spmatrix\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    553\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    554\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    555\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    556\u001b[0m         )\n\u001b[1;32m    558\u001b[0m \u001b[39mreturn\u001b[39;00m spmatrix\n",
      "File \u001b[0;32m/opt/anaconda3/envs/groupme/lib/python3.9/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Building The Model - KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import homogeneity_score\n",
    "from sklearn.metrics import completeness_score\n",
    "from sklearn.metrics import v_measure_score\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import completeness_score\n",
    "from sklearn.metrics import homogeneity_score\n",
    "from sklearn.metrics import v_measure_score\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import completeness_score\n",
    "from sklearn.metrics import homogeneity_score\n",
    "from sklearn.metrics import v_measure_score\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import completeness_score\n",
    "from sklearn.metrics import homogeneity_score\n",
    "from sklearn.metrics import v_measure_score\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import completeness_score\n",
    "from sklearn.metrics import homogeneity_score\n",
    "from sklearn.metrics import v_measure_score\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# create the model\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "# fit the model\n",
    "kmeans.fit(X_transformed)\n",
    "# get the labels\n",
    "labels = kmeans.labels_\n",
    "# get the centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "# get the inertia\n",
    "inertia = kmeans.inertia_\n",
    "# get the silhouette score\n",
    "silhouette = silhouette_score(X_transformed, labels)\n",
    "# get the calinski_harabasz score\n",
    "calinski_harabasz = calinski_harabasz_score(X_transformed, labels)\n",
    "# get the davies_bouldin score\n",
    "davies_bouldin = davies_bouldin_score(X_transformed, labels)\n",
    "# get the adjusted_rand score\n",
    "adjusted_rand = adjusted_rand_score(df['shape'], labels)\n",
    "# get the adjusted_mutual_info score\n",
    "adjusted_mutual_info = adjusted_mutual_info_score(df['shape'], labels)\n",
    "# get the homogeneity score\n",
    "homogeneity = homogeneity_score(df['shape'], labels)\n",
    "# get the completeness score\n",
    "completeness = completeness_score(df['shape'], labels)\n",
    "# get the v_measure score\n",
    "v_measure = v_measure_score(df['shape'], labels)\n",
    "# get the fowlkes_mallows score\n",
    "fowlkes_mallows = fowlkes_mallows_score(df['shape'], labels)\n",
    "# get the silhouette_samples\n",
    "silhouette_samples = silhouette_samples(X_transformed, labels)\n",
    "# get the pairwise_distances\n",
    "pairwise_distances = pairwise_distances(X_transformed, centroids)\n",
    "\n",
    "# save kmeans model as pickle file in models folder\n",
    "import pickle\n",
    "pickle.dump(kmeans, open('models/kmeans.pkl', 'wb'))\n",
    "# save the resulting scores to a csv file in the data folder\n",
    "import pandas as pd\n",
    "scores = pd.DataFrame({\n",
    "    'silhouette': [silhouette],\n",
    "    'calinski_harabasz': [calinski_harabasz],\n",
    "    'davies_bouldin': [davies_bouldin],\n",
    "    'adjusted_rand': [adjusted_rand],\n",
    "    'adjusted_mutual_info': [adjusted_mutual_info],\n",
    "    'homogeneity': [homogeneity],\n",
    "    'completeness': [completeness],\n",
    "    'v_measure': [v_measure],\n",
    "    'fowlkes_mallows': [fowlkes_mallows],\n",
    "})\n",
    "scores.to_csv('data/kmeans_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building The Model - DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# create the model\n",
    "dbscan = DBSCAN()\n",
    "# fit the model\n",
    "dbscan.fit(X_transformed)\n",
    "# get the labels\n",
    "labels = dbscan.labels_\n",
    "# get the silhouette score\n",
    "silhouette = silhouette_score(X_transformed, labels)\n",
    "# get the calinski_harabasz score\n",
    "calinski_harabasz = calinski_harabasz_score(X_transformed, labels)\n",
    "# get the davies_bouldin score\n",
    "davies_bouldin = davies_bouldin_score(X_transformed, labels)\n",
    "# get the adjusted_rand score\n",
    "adjusted_rand = adjusted_rand_score(df['shape'], labels)\n",
    "# get the adjusted_mutual_info score\n",
    "adjusted_mutual_info = adjusted_mutual_info_score(df['shape'], labels)\n",
    "# get the homogeneity score\n",
    "homogeneity = homogeneity_score(df['shape'], labels)\n",
    "# get the completeness score\n",
    "completeness = completeness_score(df['shape'], labels)\n",
    "# get the v_measure score\n",
    "v_measure = v_measure_score(df['shape'], labels)\n",
    "# get the fowlkes_mallows score\n",
    "fowlkes_mallows = fowlkes_mallows_score(df['shape'], labels)\n",
    "# get the silhouette_samples\n",
    "silhouette_samples = silhouette_samples(X_transformed, labels)\n",
    "\n",
    "# save dbscan model as pickle file in models folder\n",
    "import pickle\n",
    "pickle.dump(dbscan, open('models/dbscan.pkl', 'wb'))\n",
    "# save the resulting scores to a csv file in the data folder\n",
    "import pandas as pd\n",
    "scores = pd.DataFrame({\n",
    "    'silhouette': [silhouette],\n",
    "    'calinski_harabasz': [calinski_harabasz],\n",
    "    'davies_bouldin': [davies_bouldin],\n",
    "    'adjusted_rand': [adjusted_rand],\n",
    "    'adjusted_mutual_info': [adjusted_mutual_info],\n",
    "    'homogeneity': [homogeneity],\n",
    "    'completeness': [completeness],\n",
    "    'v_measure': [v_measure],\n",
    "    'fowlkes_mallows': [fowlkes_mallows],\n",
    "})\n",
    "scores.to_csv('data/dbscan_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the preprocessed data in the data/mufon_data_preprocessed.csv file for this step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use libraries such as scikit-learn, and KMeans to cluster the data using different features like location, time, shape, and even the description. Also, you may consider using DBSCAN for density-based clustering and Affinity Propagation for data with many clusters.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('groupme')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28dd76f97a2595215b3511d9563b8125e93469ee739d17a6b25584482d270cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
