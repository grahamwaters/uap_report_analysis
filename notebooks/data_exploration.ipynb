{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# df = pd.read_csv('../data/ufo_clean.csv', low_memory=False)\n",
    "df = pd.read_csv('../data/processed/ufos_processed.csv', low_memory=False, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 79614 entries, 0 to 79613\n",
      "Data columns (total 29 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   datetime                   79614 non-null  object \n",
      " 1   city                       79614 non-null  object \n",
      " 2   shape                      77738 non-null  object \n",
      " 3   duration (seconds)         79614 non-null  object \n",
      " 4   duration (hours/min)       79614 non-null  object \n",
      " 5   comments                   79614 non-null  object \n",
      " 6   date posted                79614 non-null  object \n",
      " 7   latitude                   79614 non-null  float64\n",
      " 8   longitude                  79614 non-null  float64\n",
      " 9   comments_shapes            79614 non-null  object \n",
      " 10  calculated_duration        79614 non-null  object \n",
      " 11  duration_value             76071 non-null  float64\n",
      " 12  duration_unit              79614 non-null  object \n",
      " 13  total_seconds              73394 non-null  float64\n",
      " 14  comments_clean             79612 non-null  object \n",
      " 15  year                       79614 non-null  int64  \n",
      " 16  month                      79614 non-null  int64  \n",
      " 17  day                        79614 non-null  int64  \n",
      " 18  hour                       79614 non-null  int64  \n",
      " 19  minute                     79614 non-null  int64  \n",
      " 20  second                     79614 non-null  int64  \n",
      " 21  date_posted                79614 non-null  object \n",
      " 22  time_since_event           79614 non-null  object \n",
      " 23  comment_length_words       79614 non-null  int64  \n",
      " 24  comment_length_characters  79614 non-null  int64  \n",
      " 25  comment_unique_words       79614 non-null  int64  \n",
      " 26  flesch_kincaid_score       79614 non-null  float64\n",
      " 27  comments_colors            79614 non-null  object \n",
      " 28  comments_modecolor         79614 non-null  object \n",
      "dtypes: float64(5), int64(9), object(15)\n",
      "memory usage: 17.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key question: For a report `r` and a geographic valence `v` (measured in km), we need to be able to assign a score to `r` that communicates the following information:\n",
    "- how many other reports `r'` are within `v` km of `r`?\n",
    "- how many of those reports have a `datetime` that is within `v_t` of the `datetime` of `r`? (where `v_t` is a time valence measured in hours).\n",
    "- how many of the reports that are within `v_t` of the `datetime` of `r` reported the same:\n",
    "  -  `comments_shapes` (the shape of the UAP from the commentary in the report - e.g. \"triangle\", \"cigar\", \"sphere\", etc.)\n",
    "  -  `comments_colors` (the color of the UAP from the commentary in the report - e.g. \"red\", \"green\", \"blue\", etc.)\n",
    "  -  `calculated_duration` (measured in seconds)\n",
    "  -  `number_of_objects` (the number of objects in the UAP) (this is a bit of a stretch, but it's worth a shot) for example, if `r` is a report that describes a single red sphere that was observed for 10 seconds, then `r'` is interesting and should be counted if it either describes a red sphere, a single red object, or a single object that was observed for 10 seconds (or any combination of those).\n",
    "\n",
    "The first step is to create a function that will take a report `r` and a valence `v` and return a score that communicates the information above. We'll call this function `score_report`.\n",
    "\n",
    "```python\n",
    "def score_report(r, v):\n",
    "    \"\"\"Score a report `r` based on the number of other reports that are within `v` km of `r` and within `v_t` hours of the `datetime` of `r`.\"\"\"\n",
    "    report_score = 0 # initialize the score to 0\n",
    "    # get the latitude and longitude of the report\n",
    "    lat = r['latitude']\n",
    "    lon = r['longitude']\n",
    "    # get the datetime of the report\n",
    "    dt = r['datetime']\n",
    "    # get the comments_shapes, comments_colors, calculated_duration, and number_of_objects of the report\n",
    "    comments_shapes = r['comments_shapes']\n",
    "    comments_colors = r['comments_colors']\n",
    "    calculated_duration = r['calculated_duration']\n",
    "    number_of_objects = r['number_of_objects']\n",
    "    # get the reports that are within `v` km of `r`\n",
    "    nearby_reports = reports[reports.apply(lambda x: haversine((x['latitude'], x['longitude']), (lat, lon)) <= v, axis=1)]\n",
    "    # get the reports that are within `v_t` hours of the `datetime` of `r`\n",
    "    keeper_reports = nearby_reports[nearby_reports.apply(lambda x: abs((x['datetime'] - dt).total_seconds()) <= v_t, axis=1)]\n",
    "    keepers = {} # initialize a dictionary to keep track of the reports that we want to keep\n",
    "    # get the reports that have some combination of the same `comments_shapes`, `comments_colors`, `calculated_duration`, and `number_of_objects` as `r`\n",
    "    combinations = list(itertools.product(comments_shapes, comments_colors, calculated_duration, number_of_objects))\n",
    "    for combination in combinations:\n",
    "        keep_set = keeper_reports[keeper_reports.apply(lambda x: combination[0] in x['comments_shapes'] and combination[1] in x['comments_colors'] and combination[2] in x['calculated_duration'] and combination[3] in x['number_of_objects'], axis=1)]\n",
    "        # save the keep_set to the keepers dictionary as a key-value pair where the key is the combination and the value is the keep_set\n",
    "        keepers[combination] = keep_set\n",
    "    # add the number of reports in each keep_set to the report_score\n",
    "    for keep_set in keepers.values():\n",
    "        report_score += len(keep_set)\n",
    "    return report_score, keepers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def haversine(p1, p2):\n",
    "    \"\"\"Calculate the great circle distance between two points on the earth (specified in decimal degrees)\"\"\"\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [p1[1], p1[0], p2[1], p2[0]])\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    # Radius of earth in kilometers is 6371\n",
    "    km = 6371* c\n",
    "    return km\n",
    "\n",
    "def score_one_report(r, v, v_t, df):\n",
    "    \"\"\"Assign a float type score to a report `r` based on (1) the number of other reports that are within `v` km of `r` AND (2) within `v_t` hours of the `datetime` of `r`. If a report `r'` within `v` km of `r` contains any of the same `comments_shapes`, `comments_colors`, `calculated_duration`, and `number_of_objects` as `r`, then add 1 to the score and save `r'` to a dictionary of reports that we want to keep.\n",
    "    To get the distance between two points, use the haversine function. To get the difference between two datetimes, use the `total_seconds` method.\n",
    "    \"\"\"\n",
    "\n",
    "    report_score = 0 # initialize the score to 0\n",
    "    # get the index of the current report\n",
    "    report_index = r.name\n",
    "    # get the latitude and longitude of the report\n",
    "    lat = r['latitude']\n",
    "    lon = r['longitude']\n",
    "    # get the datetime of the report\n",
    "    dt = r['datetime']\n",
    "    # get the comments_shapes, comments_colors, calculated_duration, and number_of_objects of the report\n",
    "    comments_shapes = r['comments_shapes']\n",
    "    comments_colors = r['comments_colors']\n",
    "    calculated_duration = r['calculated_duration']\n",
    "    number_of_objects = r['number_of_objects']\n",
    "    # get the indices of the reports that are within `v` km of `r`\n",
    "    nearby_report_indices = np.where(distances[report_index] <= v)[0]\n",
    "    # get the reports that are within `v` km of `r`\n",
    "    nearby_reports = df.iloc[nearby_report_indices]\n",
    "    # get the reports that are within `v_t` hours of the `datetime` of `r`\n",
    "    keeper_reports = nearby_reports[nearby_reports.apply(lambda x: abs((x['datetime'] - dt).total_seconds()) <= v_t, axis=1)]\n",
    "    keepers = {} # initialize a dictionary to keep track of the reports that we want to keep\n",
    "    # get the reports that have some combination of the same `comments_shapes`, `comments_colors`, `calculated_duration`, and `number_of_objects` as `r`\n",
    "    combinations = list(itertools.product(comments_shapes, comments_colors, calculated_duration, number_of_objects))\n",
    "    print(len(combinations), ' combinations to check')\n",
    "    for combination in combinations:\n",
    "        keep_set = keeper_reports[keeper_reports.apply(lambda x: combination[0] in x['comments_shapes'] and combination[1] in x['comments_colors'] and combination[2] in x['calculated_duration'] and combination[3] in x['number_of_objects'], axis=1)]\n",
    "        # save the keep_set to the keepers dictionary as a key-value pair where the key is the combination and the value is the keep_set\n",
    "        keepers[combination] = keep_set\n",
    "        # add the number of reports in the keep_set to the report_score\n",
    "        report_score += len(keep_set)\n",
    "    return report_score, keepers\n",
    "\n",
    "def score_all_reports(df, v, v_t):\n",
    "    \"\"\"Score all of the reports in `df` based on the number of other reports that are within `v` km of `r` and within `v_t` hours of the `datetime` of `r`.\"\"\"\n",
    "    report_scores = [] # initialize a list to keep track of the report scores\n",
    "    keepers = {} # initialize a dictionary to keep track of the reports that we want to keep\n",
    "    for i, r in df.iterrows():\n",
    "        report_score, keep_set = score_one_report(r, v, v_t, df, distances)\n",
    "        report_scores.append(report_score)\n",
    "        keepers[i] = keep_set\n",
    "    return report_scores, keepers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "score_one_report() missing 1 required positional argument: 'distances'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m score, keepers \u001b[38;5;241m=\u001b[39m \u001b[43mscore_all_reports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# score all of the reports in `df` based on the number of other reports that are within 10 km of `r` and within 24 hours of the `datetime` of `r`\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe maximum score is\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mmax\u001b[39m(score)) \u001b[38;5;66;03m# print the maximum score\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe minimum score is\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mmin\u001b[39m(score)) \u001b[38;5;66;03m# print the minimum score\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m, in \u001b[0;36mscore_all_reports\u001b[0;34m(df, v, v_t)\u001b[0m\n\u001b[1;32m     52\u001b[0m keepers \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;66;03m# initialize a dictionary to keep track of the reports that we want to keep\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 54\u001b[0m     report_score, keep_set \u001b[38;5;241m=\u001b[39m \u001b[43mscore_one_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     report_scores\u001b[38;5;241m.\u001b[39mappend(report_score)\n\u001b[1;32m     56\u001b[0m     keepers[i] \u001b[38;5;241m=\u001b[39m keep_set\n",
      "\u001b[0;31mTypeError\u001b[0m: score_one_report() missing 1 required positional argument: 'distances'"
     ]
    }
   ],
   "source": [
    "score, keepers = score_all_reports(df, 10, 24*60*60) # score all of the reports in `df` based on the number of other reports that are within 10 km of `r` and within 24 hours of the `datetime` of `r`\n",
    "print('The maximum score is', max(score)) # print the maximum score\n",
    "print('The minimum score is', min(score)) # print the minimum score\n",
    "print('The mean score is', np.mean(score)) # print the mean score\n",
    "print('The median score is', np.median(score)) # print the median score\n",
    "print(f'The number of reports with a score of 0 is {score.count(0)}') # print the number of reports with a score of 0\n",
    "print(len(keepers), ' reports scored')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.datasets.samples_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DBSCAN\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamples_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_blobs\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# DBSCAN clustering on the data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Standardize the data to have a mean of ~0 and a variance of 1\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.datasets.samples_generator'"
     ]
    }
   ],
   "source": [
    "# cluster the reports by the grade level of the witness\n",
    "df['grade_level'] = df['grade_level'].astype('category')\n",
    "df['grade_level'] = df['grade_level'].cat.codes\n",
    "df['grade_level'] = df['grade_level'].astype('category')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('groupme')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28dd76f97a2595215b3511d9563b8125e93469ee739d17a6b25584482d270cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
